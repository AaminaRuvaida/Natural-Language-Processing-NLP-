{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Tool Kit (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\aamina\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\aamina\\anaconda3\\lib\\site-packages (from nltk) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aamina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='I am Learning Natural Language Processing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'Learning', 'Natural', 'Language', 'Processing']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Ramya is very good in Math. She is smart in the class.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ramya is very good in Math.', 'She is smart in the class.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"NLA is fun and interesting, It deals with text, sounds but can't deal with images. We have session for NLP @12AM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'fun',\n",
       " 'and',\n",
       " 'interesting',\n",
       " 't',\n",
       " 'deals',\n",
       " 'with',\n",
       " 'text',\n",
       " 'sounds',\n",
       " 'but',\n",
       " 'can',\n",
       " 't',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'images',\n",
       " 'e',\n",
       " 'have',\n",
       " 'session',\n",
       " 'for']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print all word by word start with Lower case a-z\n",
    "regexp_tokenize(text,\"[a-z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'fun',\n",
       " 'and',\n",
       " 'interesting',\n",
       " 't',\n",
       " 'deals',\n",
       " 'with',\n",
       " 'text',\n",
       " 'sounds',\n",
       " 'but',\n",
       " \"can't\",\n",
       " 'deal',\n",
       " 'with',\n",
       " 'images',\n",
       " 'e',\n",
       " 'have',\n",
       " 'session',\n",
       " 'for']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Include  Extra quote ' in the sentence\n",
    "regexp_tokenize(text,\"[a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLA', 'I', 'W', 'NLP', 'AM']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print all word by word start with Upper case A-Z\n",
    "regexp_tokenize(text,\"[A-Z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"NLA is fun and interesting, It deals with text, sounds but can't deal with images. We have session for NLP @12AM\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print Everything in One line\n",
    "\n",
    "regexp_tokenize(text,\"[\\a-z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLA ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ', I',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ', ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '. W',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' NLP @12AM']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Anything Start with ^ carat is not equal\n",
    "\n",
    "regexp_tokenize(text,\"[^a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print only Number in the sentence\n",
    "regexp_tokenize(text,\"[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"NLA is fun and interesting, It deals with text, sounds but can't deal with images. We have session for NLP @\",\n",
       " 'AM']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print excluding the number\n",
    "regexp_tokenize(text,\"[^0-9]+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 is excluded in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print only symbol that is present in the sentence\n",
    "regexp_tokenize(text,\"[@]+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aamina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way to perform the stop words task\n",
    "\n",
    "stop_set=set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding more stopwords\n",
    "\n",
    "stop_set.update(('old','new'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'was', 'only', 'be', \"don't\", 'should', 'over', 'hadn', 'herself', 'any', 'most', 'a', 'are', 'couldn', 'once', 'when', 'isn', 'the', \"wouldn't\", 'it', 'doing', \"didn't\", 'm', 'same', \"you'd\", 'other', 'so', 'until', \"shouldn't\", 'before', 'new', 'mustn', 'up', \"won't\", 'about', 'itself', \"mustn't\", \"she's\", \"isn't\", 'against', 'theirs', 'him', 'at', 'after', 'out', 'own', 'as', \"doesn't\", 'these', 'shan', 'y', 'do', 'too', 'don', 'am', \"needn't\", 'into', 'ours', 'she', 'her', 'being', 'by', 'our', 'again', 'my', 'has', 'hasn', \"weren't\", 'between', 'in', 'had', 'll', 'than', 'shouldn', 'down', 't', 'd', 'not', 'i', 'of', 'o', 'we', \"you'll\", 'can', 'off', 'very', 'no', 'now', 'doesn', 'here', \"you've\", 'then', 'who', 'if', 'from', 'ma', 'been', 'some', 'on', 'them', 'where', 'both', 'having', 'there', 'under', 'yourselves', 'above', 'why', 'for', 'but', 'whom', \"hadn't\", 'such', \"mightn't\", 'themselves', \"you're\", 'is', 's', 'needn', 'they', \"should've\", 'did', 'its', 'mightn', 'old', 'their', 'didn', 'wouldn', \"that'll\", 'haven', 'those', 'ourselves', 'during', 'you', 'he', \"aren't\", 'yourself', 'an', 'which', 'what', 'all', 'aren', 'few', 'each', 'further', 'or', 'weren', 'have', 'ain', 'to', 'more', \"hasn't\", 'were', 'his', 'just', 'yours', 'with', 'himself', 'your', \"it's\", 'below', 'through', 'how', 'while', 'myself', 're', 'nor', 'will', 'this', 'and', 'hers', 've', \"haven't\", 'that', \"wasn't\", 'because', 'won', 'wasn', 'me', \"shan't\", \"couldn't\", 'does'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also ignore punctuation from the sentence\n",
    "\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Kalam was elected as the 11th president of India in 2002 with the support of both the ruling Bharatiya Janata Party and the then-opposition Indian National Congress. Widely referred to as the People's President, he returned to his civilian life of education, writing and public service after a single term. He was a recipient of several prestigious awards, including the Bharat Ratna.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original counts: 384\n",
      "Cleaned data counts: 37\n",
      "\n",
      " ['Kalam', 'elected', '11th', 'president', 'India', '2002', 'support', 'ruling', 'Bharatiya', 'Janata', 'Party', 'then-opposition', 'Indian', 'National', 'Congress', 'Widely', 'referred', 'People', \"'s\", 'President', 'returned', 'civilian', 'life', 'education', 'writing', 'public', 'service', 'single', 'term', 'He', 'recipient', 'several', 'prestigious', 'awards', 'including', 'Bharat', 'Ratna']\n"
     ]
    }
   ],
   "source": [
    "cleaned_data=[]\n",
    "\n",
    "for word in nltk.word_tokenize(text):\n",
    "    if word not in punct:\n",
    "        if word not in stop_words:\n",
    "            cleaned_data.append(word)\n",
    "            \n",
    "print(\"Original counts:\",len(text))\n",
    "print(\"Cleaned data counts:\",len(cleaned_data))\n",
    "print('\\n',cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sentence we have ignored the punctuation and stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "***************\n",
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n",
      "promot\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Lancaster Stemmer\n",
      "***************\n",
      "hobby\n",
      "hobby\n",
      "comput\n",
      "comput\n",
      "promot\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Snowball Stemmer\n",
      "***************\n",
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n",
      "promot\n"
     ]
    }
   ],
   "source": [
    "lancaster=LancasterStemmer()\n",
    "porter=PorterStemmer()\n",
    "snowball=SnowballStemmer('english')\n",
    "\n",
    "#Porter Stemmer\n",
    "print('Porter Stemmer')\n",
    "print(\"***************\")\n",
    "print(porter.stem(\"hobby\"))\n",
    "print(porter.stem(\"hobbies\"))\n",
    "print(porter.stem(\"computer\"))\n",
    "print(porter.stem(\"computation\"))\n",
    "print(porter.stem(\"promotion\"))\n",
    "\n",
    "print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "#Lancaster Stemmer\n",
    "print('Lancaster Stemmer')\n",
    "print(\"***************\")\n",
    "print(lancaster.stem(\"hobby\"))\n",
    "print(lancaster.stem(\"hobbies\"))\n",
    "print(lancaster.stem(\"computer\"))\n",
    "print(lancaster.stem(\"computation\"))\n",
    "print(lancaster.stem(\"promotion\"))\n",
    "\n",
    "print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "\n",
    "#Snowball Stemmer\n",
    "print('Snowball Stemmer')\n",
    "print(\"***************\")\n",
    "print(snowball.stem(\"hobby\"))\n",
    "print(snowball.stem(\"hobbies\"))\n",
    "print(snowball.stem(\"computer\"))\n",
    "print(snowball.stem(\"computation\"))\n",
    "print(snowball.stem(\"promotion\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aamina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "run\n",
      "ran\n",
      "go\n",
      "going\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "print(lemma.lemmatize('running'))\n",
    "print(lemma.lemmatize('runs'))\n",
    "print(lemma.lemmatize('ran'))\n",
    "print(lemma.lemmatize('go'))\n",
    "print(lemma.lemmatize('going'))\n",
    "print(lemma.lemmatize('goes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "run\n",
      "go\n",
      "go\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize('running',pos='v'))\n",
    "print(lemma.lemmatize('runs',pos='v'))\n",
    "print(lemma.lemmatize('ran',pos='v'))\n",
    "print(lemma.lemmatize('go',pos='v'))\n",
    "print(lemma.lemmatize('going',pos='v'))\n",
    "print(lemma.lemmatize('goes',pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Bring King Bing Sing Ring Anthing Nothing Something Thing Doing Enjoying\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for Bring is bring\n",
      "Stemming for King is king\n",
      "Stemming for Bing is bing\n",
      "Stemming for Sing is sing\n",
      "Stemming for Ring is ring\n",
      "Stemming for Anthing is anth\n",
      "Stemming for Nothing is noth\n",
      "Stemming for Something is someth\n",
      "Stemming for Thing is thing\n",
      "Stemming for Doing is do\n",
      "Stemming for Enjoying is enjoy\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "\n",
    "porter_stemmer=PorterStemmer()\n",
    "\n",
    "tokenization=nltk.word_tokenize(text)\n",
    "\n",
    "for w in tokenization:\n",
    "    print(\"Stemming for {} is {}\".format(w, porter_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for Bring is Bring\n",
      "Lemma for King is King\n",
      "Lemma for Bing is Bing\n",
      "Lemma for Sing is Sing\n",
      "Lemma for Ring is Ring\n",
      "Lemma for Anthing is Anthing\n",
      "Lemma for Nothing is Nothing\n",
      "Lemma for Something is Something\n",
      "Lemma for Thing is Thing\n",
      "Lemma for Doing is Doing\n",
      "Lemma for Enjoying is Enjoying\n"
     ]
    }
   ],
   "source": [
    "#Lemmatizer\n",
    "\n",
    "word_lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "tokenization=nltk.word_tokenize(text)\n",
    "\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, word_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms==> {'participating', 'alive', 'fighting', 'active_agent', 'combat-ready', 'dynamic', 'active', 'active_voice'}\n",
      "Antonyms==> {'passive_voice', 'dormant', 'quiet', 'stative', 'extinct', 'passive', 'inactive'}\n"
     ]
    }
   ],
   "source": [
    "synonyms=[]\n",
    "antonyms=[]\n",
    "\n",
    "for syn in wordnet.synsets(\"active\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "print('Synonyms==>',set(synonyms))\n",
    "print('Antonyms==>',set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms==> {'slumber', 'kip', 'eternal_rest', 'nap', 'sopor', 'sleep', 'rest', 'eternal_sleep', \"catch_some_Z's\", \"log_Z's\", 'quietus'}\n",
      "Antonyms==> {'wake'}\n"
     ]
    }
   ],
   "source": [
    "synonyms=[]\n",
    "antonyms=[]\n",
    "\n",
    "for syn in wordnet.synsets(\"Sleep\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "print('Synonyms==>',set(synonyms))\n",
    "print('Antonyms==>',set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms==> {'near', 'thoroughly', 'sound', 'good', 'just', 'serious', 'honorable', 'unspoiled', 'safe', 'practiced', 'trade_good', 'full', 'undecomposed', 'honest', 'beneficial', 'skilful', 'adept', 'expert', 'skillful', 'effective', 'upright', 'secure', 'in_effect', 'estimable', 'soundly', 'respectable', 'right', 'proficient', 'goodness', 'ripe', 'well', 'unspoilt', 'salutary', 'dear', 'commodity', 'in_force', 'dependable'}\n",
      "Antonyms==> {'evilness', 'badness', 'evil', 'ill', 'bad'}\n"
     ]
    }
   ],
   "source": [
    "synonyms=[]\n",
    "antonyms=[]\n",
    "\n",
    "for syn in wordnet.synsets(\"Good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "print('Synonyms==>',set(synonyms))\n",
    "print('Antonyms==>',set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Count Vectorization BOW, n_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words: ['an', 'bag', 'example', 'is', 'of', 'this', 'words']\n"
     ]
    }
   ],
   "source": [
    "string=[\"This is an example of Bag of words\"]\n",
    "vect1=CountVectorizer()\n",
    "vect1.fit_transform(string)\n",
    "print(\"Bag of Words:\",vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 5, 'is': 3, 'an': 0, 'example': 2, 'of': 4, 'bag': 1, 'words': 6}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect1.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Present at: [[1 1 1 1 1 1 1]]\n",
      "Original Text: ['an', 'bag', 'example', 'is', 'of', 'this', 'words']\n"
     ]
    }
   ],
   "source": [
    "vect2=CountVectorizer()\n",
    "vect2.fit(string)\n",
    "string2=['Lets understand the Bag of Words now']\n",
    "c_new_vect=vect2.fit_transform(string2)\n",
    "\n",
    "print(\"Text Present at:\",c_new_vect.toarray())\n",
    "print(\"Original Text:\",vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words using Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
      "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
      "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
      "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
      "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
      "                            'itself', ...])\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words using Stop Words\n",
    "\n",
    "stopword=stopwords.words('english')\n",
    "string=['This is an example of Bag of words using the stopwords']\n",
    "stop_vect=CountVectorizer(stop_words=stopword)\n",
    "print(stop_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words: ['bag', 'example', 'stopwords', 'using', 'words']\n",
      "Vocabulary: {'example': 1, 'bag': 0, 'words': 4, 'using': 3, 'stopwords': 2}\n"
     ]
    }
   ],
   "source": [
    "stop_vect.fit_transform(string)\n",
    "print(\"Bag of Words:\",stop_vect.get_feature_names())\n",
    "print(\"Vocabulary:\",stop_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words using Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_matrix(message, countvect):\n",
    "    term_vect=countvect.fit_transform(message)\n",
    "    return pd.DataFrame(term_vect.toarray(),columns=countvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>be</th>\n",
       "      <th>become</th>\n",
       "      <th>but</th>\n",
       "      <th>consistent</th>\n",
       "      <th>in</th>\n",
       "      <th>learning</th>\n",
       "      <th>master</th>\n",
       "      <th>nlp</th>\n",
       "      <th>practise</th>\n",
       "      <th>soon</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  be  become  but  consistent  in  learning  master  nlp  practise  \\\n",
       "0    1   0       0    0           0   0         1       0    1         0   \n",
       "1    0   1       0    0           0   0         0       0    0         0   \n",
       "2    0   0       1    1           1   1         0       1    1         1   \n",
       "\n",
       "   soon  we  will  \n",
       "0     0   1     0  \n",
       "1     1   1     1  \n",
       "2     0   0     1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message=['We are learning NLP', 'Soon we will be', 'But consistent practise will become master in NLP']\n",
    "\n",
    "vect=CountVectorizer()\n",
    "text_matrix(message,vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: ['an', 'example', 'is', 'n_grams', 'of', 'this']\n",
      "2-gram: ['an example', 'example of', 'is an', 'of n_grams', 'this is']\n",
      "3-gram: ['an example of', 'example of n_grams', 'is an example', 'this is an']\n",
      "4-gram: ['an example of n_grams', 'is an example of', 'this is an example']\n"
     ]
    }
   ],
   "source": [
    "vect=['This is an example of n_grams']\n",
    "\n",
    "vect1=CountVectorizer(ngram_range=(1,1))\n",
    "vect1.fit_transform(vect)\n",
    "\n",
    "vect2=CountVectorizer(ngram_range=(2,2))\n",
    "vect2.fit_transform(vect)\n",
    "\n",
    "vect3=CountVectorizer(ngram_range=(3,3))\n",
    "vect3.fit_transform(vect)\n",
    "\n",
    "vect4=CountVectorizer(ngram_range=(4,4))\n",
    "vect4.fit_transform(vect)\n",
    "\n",
    "print(\"1-gram:\",vect1.get_feature_names())\n",
    "print(\"2-gram:\",vect2.get_feature_names())\n",
    "print(\"3-gram:\",vect3.get_feature_names())\n",
    "print(\"4-gram:\",vect4.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Termed Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>example</th>\n",
       "      <th>how</th>\n",
       "      <th>interesting</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>journey</th>\n",
       "      <th>let</th>\n",
       "      <th>nlp</th>\n",
       "      <th>see</th>\n",
       "      <th>this</th>\n",
       "      <th>towards</th>\n",
       "      <th>works</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.393470</td>\n",
       "      <td>0.587521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.587521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.289662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432518</td>\n",
       "      <td>0.289662</td>\n",
       "      <td>0.289662</td>\n",
       "      <td>0.432518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432518</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         an   example       how  interesting        is        it   journey  \\\n",
       "0  0.393470  0.587521  0.000000     0.000000  0.393470  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.474125     0.000000  0.000000  0.317527  0.000000   \n",
       "2  0.289662  0.000000  0.000000     0.432518  0.289662  0.289662  0.432518   \n",
       "\n",
       "        let       nlp       see      this   towards     works  \n",
       "0  0.000000  0.000000  0.000000  0.587521  0.000000  0.000000  \n",
       "1  0.474125  0.000000  0.474125  0.000000  0.000000  0.474125  \n",
       "2  0.000000  0.432518  0.000000  0.000000  0.432518  0.000000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf=TfidfVectorizer(smooth_idf=False)\n",
    "\n",
    "doc=[\"This is an example\",\n",
    "    \"Let's see how it works\",\n",
    "    \"It is an interesting journey towards NLP\"]\n",
    "\n",
    "doc_vect=tfidf.fit_transform(doc)\n",
    "df=pd.DataFrame(doc_vect.todense(),columns=tfidf.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfIdf Using a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_matrix(message, countvect):\n",
    "    term_doc=countvect.fit_transform(message)\n",
    "    return pd.DataFrame(term_doc.toarray(),columns=countvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>calm</th>\n",
       "      <th>interesting</th>\n",
       "      <th>is</th>\n",
       "      <th>make</th>\n",
       "      <th>mind</th>\n",
       "      <th>peace</th>\n",
       "      <th>place</th>\n",
       "      <th>such</th>\n",
       "      <th>this</th>\n",
       "      <th>to</th>\n",
       "      <th>visit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.402040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355432</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.467351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  beautiful      calm  interesting        is      make      mind  \\\n",
       "0  0.000000   0.528635  0.000000     0.000000  0.528635  0.000000  0.000000   \n",
       "1  0.000000   0.000000  0.000000     0.467351  0.000000  0.000000  0.000000   \n",
       "2  0.447214   0.000000  0.447214     0.000000  0.000000  0.447214  0.447214   \n",
       "\n",
       "      peace     place      such      this        to     visit  \n",
       "0  0.000000  0.402040  0.000000  0.528635  0.000000  0.000000  \n",
       "1  0.000000  0.355432  0.467351  0.000000  0.467351  0.467351  \n",
       "2  0.447214  0.000000  0.000000  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message=[\"This place is beautiful\",\n",
    "        \"Interesting to visit such place\",\n",
    "        \"Make mind peace and calm\"]\n",
    "\n",
    "tfidf=TfidfVectorizer()\n",
    "text_matrix(message,tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer, TfidfVectorizer, ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names:\n",
      " ['by car', 'by jack', 'car was', 'cleaned by', 'jack was', 'was cleaned']\n",
      "Array:\n",
      " [[0 1 1 1 0 1]\n",
      " [1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "array=[\"Car was cleaned by Jack\",\n",
    "      \"Jack was cleaned by Car\"]\n",
    "\n",
    "count_vect=CountVectorizer(ngram_range=(2,2))\n",
    "x=count_vect.fit_transform(array)\n",
    "\n",
    "print(\"Feature Names:\\n\",count_vect.get_feature_names())\n",
    "print(\"Array:\\n\",x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names:\n",
      " ['and', 'beautiful', 'calm', 'interesting', 'is', 'make', 'mind', 'peace', 'place', 'such', 'this', 'to', 'visit']\n",
      "Array:\n",
      " [[0.         0.57615236 0.57615236 0.40993715 0.         0.40993715]\n",
      " [0.57615236 0.         0.         0.40993715 0.57615236 0.40993715]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect=TfidfVectorizer(ngram_range=(2,2))\n",
    "x=tfidf_vect.fit_transform(array)\n",
    "print(\"Feature Names:\\n\",tfidf.get_feature_names())\n",
    "print(\"Array:\\n\",x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names:\n",
      " ['and', 'beautiful', 'calm', 'interesting', 'is', 'make', 'mind', 'peace', 'place', 'such', 'this', 'to', 'visit']\n",
      "Array:\n",
      " [[0.         1.40546511 1.40546511 1.         0.         1.        ]\n",
      " [1.40546511 0.         0.         1.         1.40546511 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Making the Normalization to None\n",
    "tfidf_vect=TfidfVectorizer(ngram_range=(2,2),norm=None)\n",
    "x=tfidf_vect.fit_transform(array)\n",
    "print(\"Feature Names:\\n\",tfidf.get_feature_names())\n",
    "print(\"Array:\\n\",x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aamina\\anaconda3\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp37-cp37m-win_amd64.whl (24.0 MB)\n",
      "     -------------------------------------- 24.0/24.0 MB 625.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\aamina\\anaconda3\\lib\\site-packages (from gensim) (1.19.4)\n",
      "Collecting Cython==0.29.28\n",
      "  Downloading Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "     ------------------------------------ 983.8/983.8 kB 582.3 kB/s eta 0:00:00\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.0.0-py3-none-any.whl (58 kB)\n",
      "     -------------------------------------- 58.4/58.4 kB 280.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\aamina\\anaconda3\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.15\n",
      "    Uninstalling Cython-0.29.15:\n",
      "      Successfully uninstalled Cython-0.29.15\n",
      "Successfully installed Cython-0.29.28 gensim-4.2.0 smart-open-6.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -ffi (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\aamina\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"15 October 1931  27 July 2015) was an Indian aerospace scientist who served as the 11th President of India from 2002 to 2007. He was born and raised in Rameswaram, Tamil Nadu and studied physics and aerospace engineering. He spent the next four decades as a scientist and science administrator, mainly at the Defence Research and Development Organisation (DRDO) and Indian Space Research Organisation (ISRO) and was intimately involved in India's civilian space programme and military missile development efforts. He thus came to be known as the 'Missile Man of India' for his work on the development of ballistic missile and launch vehicle technology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=paragraph.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15 October 1931  27 July 2015 was an Indian aerospace scientist who served as the 11th President of India from 2002 to 2007 He was born and raised in Rameswaram Tamil Nadu and studied physics and aerospace engineering He spent the next four decades as a scientist and science administrator mainly at the Defence Research and Development Organisation DRDO and Indian Space Research Organisation ISRO and was intimately involved in Indias civilian space programme and military missile development efforts He thus came to be known as the Missile Man of India for his work on the development of ballistic missile and launch vehicle technology'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=re.sub(r'\\[[0-9]*\\]',' ',paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=re.sub(r'\\s+',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=re.sub(r'\\d',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=re.sub(r'\\s+',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " october  july was an indian aerospace scientist who served as the th president of india from to he was born and raised in rameswaram tamil nadu and studied physics and aerospace engineering he spent the next four decades as a scientist and science administrator mainly at the defence research and development organisation drdo and indian space research organisation isro and was intimately involved in indias civilian space programme and military missile development efforts he thus came to be known as the missile man of india for his work on the development of ballistic missile and launch vehicle technology\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' october  july was an indian aerospace scientist who served as the th president of india from to he was born and raised in rameswaram tamil nadu and studied physics and aerospace engineering he spent the next four decades as a scientist and science administrator mainly at the defence research and development organisation drdo and indian space research organisation isro and was intimately involved in indias civilian space programme and military missile development efforts he thus came to be known as the missile man of india for his work on the development of ballistic missile and launch vehicle technology']\n"
     ]
    }
   ],
   "source": [
    "#preparing the dataset\n",
    "sentences=nltk.sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['october', '', 'july', 'was', 'an', 'indian', 'aerospace', 'scientist', 'who', 'served', 'as', 'the', 'th', 'president', 'of', 'india', 'from', 'to', 'he', 'was', 'born', 'and', 'raised', 'in', 'rameswaram', 'tamil', 'nadu', 'and', 'studied', 'physics', 'and', 'aerospace', 'engineering', 'he', 'spent', 'the', 'next', 'four', 'decades', 'as', 'a', 'scientist', 'and', 'science', 'administrator', 'mainly', 'at', 'the', 'defence', 'research', 'and', 'development', 'organisation', 'drdo', 'and', 'indian', 'space', 'research', 'organisation', 'isro', 'and', 'was', 'intimately', 'involved', 'in', 'indias', 'civilian', 'space', 'programme', 'and', 'military', 'missile', 'development', 'efforts', 'he', 'thus', 'came', 'to', 'be', 'known', 'as', 'the', 'missile', 'man', 'of', 'india', 'for', 'his', 'work', 'on', 'the', 'development', 'of', 'ballistic', 'missile', 'and', 'launch', 'vehicle', 'technology']]\n"
     ]
    }
   ],
   "source": [
    "sent_word=[nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "print(sent_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc=string.punctuation\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sent_word)):\n",
    "    sent_word[i]=[word for word in stopwords.words('english') if word not in punc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the Word2Vec model\n",
    "model=Word2Vec(sent_word, min_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the index key\n",
    "words = list(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"wouldn't\",\n",
       " 'am',\n",
       " 'but',\n",
       " 'and',\n",
       " 'the',\n",
       " 'an',\n",
       " 'a',\n",
       " 'doing',\n",
       " 'did',\n",
       " 'does',\n",
       " 'do',\n",
       " 'having',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'being',\n",
       " 'been',\n",
       " 'be',\n",
       " 'were',\n",
       " 'was',\n",
       " 'are',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'between',\n",
       " 'to',\n",
       " 'below',\n",
       " 'above',\n",
       " 'after',\n",
       " 'before',\n",
       " 'during',\n",
       " 'through',\n",
       " 'into',\n",
       " 'against',\n",
       " 'as',\n",
       " 'about',\n",
       " 'with',\n",
       " 'for',\n",
       " 'by',\n",
       " 'at',\n",
       " 'of',\n",
       " 'while',\n",
       " 'until',\n",
       " 'is',\n",
       " 'those',\n",
       " 'up',\n",
       " 'these',\n",
       " 'his',\n",
       " 'him',\n",
       " 'he',\n",
       " 'yourselves',\n",
       " 'yourself',\n",
       " 'yours',\n",
       " 'your',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you've\",\n",
       " \"you're\",\n",
       " 'you',\n",
       " 'ourselves',\n",
       " 'ours',\n",
       " 'our',\n",
       " 'we',\n",
       " 'myself',\n",
       " 'my',\n",
       " 'me',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'theirs',\n",
       " \"that'll\",\n",
       " 'that',\n",
       " 'this',\n",
       " 'whom',\n",
       " 'who',\n",
       " 'which',\n",
       " 'what',\n",
       " 'themselves',\n",
       " 'their',\n",
       " 'her',\n",
       " 'them',\n",
       " 'they',\n",
       " 'itself',\n",
       " 'its',\n",
       " \"it's\",\n",
       " 'it',\n",
       " 'herself',\n",
       " 'hers',\n",
       " 'from',\n",
       " 'down',\n",
       " 'wouldn',\n",
       " \"should've\",\n",
       " 'hasn',\n",
       " \"hadn't\",\n",
       " 'hadn',\n",
       " \"doesn't\",\n",
       " 'doesn',\n",
       " \"didn't\",\n",
       " 'didn',\n",
       " \"couldn't\",\n",
       " 'couldn',\n",
       " \"aren't\",\n",
       " 'aren',\n",
       " 'ain',\n",
       " 'y',\n",
       " 've',\n",
       " 're',\n",
       " 'o',\n",
       " 'm',\n",
       " 'll',\n",
       " 'd',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " \"shan't\",\n",
       " \"won't\",\n",
       " 'won',\n",
       " \"weren't\",\n",
       " 'weren',\n",
       " \"wasn't\",\n",
       " 'wasn',\n",
       " \"shouldn't\",\n",
       " 'shouldn',\n",
       " 'shan',\n",
       " 'isn',\n",
       " \"needn't\",\n",
       " 'needn',\n",
       " \"mustn't\",\n",
       " 'mustn',\n",
       " \"mightn't\",\n",
       " 'mightn',\n",
       " 'ma',\n",
       " \"isn't\",\n",
       " 'now',\n",
       " 'should',\n",
       " 'in',\n",
       " \"don't\",\n",
       " 'each',\n",
       " 'both',\n",
       " 'any',\n",
       " 'all',\n",
       " 'how',\n",
       " 'why',\n",
       " 'where',\n",
       " 'when',\n",
       " 'there',\n",
       " 'here',\n",
       " 'once',\n",
       " 'then',\n",
       " 'further',\n",
       " 'again',\n",
       " 'under',\n",
       " 'over',\n",
       " 'off',\n",
       " 'on',\n",
       " 'out',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'than',\n",
       " 'don',\n",
       " 'just',\n",
       " 'will',\n",
       " 'can',\n",
       " 't',\n",
       " 's',\n",
       " 'very',\n",
       " 'too',\n",
       " 'so',\n",
       " 'other',\n",
       " 'same',\n",
       " 'own',\n",
       " 'only',\n",
       " 'not',\n",
       " 'nor',\n",
       " 'no',\n",
       " 'such',\n",
       " 'some',\n",
       " 'i']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00479909, -0.00360642, -0.00427806,  0.00119771, -0.0040969 ,\n",
       "       -0.00565702,  0.00314622,  0.00836692, -0.00654244, -0.00911102,\n",
       "       -0.00333786,  0.00750627, -0.00970629,  0.00961928, -0.00042636,\n",
       "        0.00231826, -0.00185181, -0.00578817,  0.00308253,  0.00616009,\n",
       "       -0.00706141, -0.00116217,  0.00153953,  0.00424454,  0.00713913,\n",
       "       -0.00353377,  0.00728318, -0.00559547, -0.00221704,  0.00909163,\n",
       "        0.00541548, -0.00849885, -0.00166197, -0.00886247, -0.00166174,\n",
       "        0.00568254, -0.0074076 ,  0.00551872,  0.00609869, -0.00379928,\n",
       "       -0.00964687, -0.00251578, -0.00255666,  0.0037395 ,  0.00894543,\n",
       "        0.00199653, -0.00211153,  0.00297103, -0.00677508, -0.00128476,\n",
       "       -0.00148038,  0.00959412, -0.00580136, -0.00701747,  0.00236061,\n",
       "        0.00264868, -0.00714834, -0.00587657, -0.00119592, -0.00331328,\n",
       "       -0.00896841, -0.00404798, -0.00296044,  0.0065058 ,  0.00097004,\n",
       "        0.00160853,  0.00027271, -0.00337703,  0.00309522,  0.00073724,\n",
       "        0.00598971, -0.00076678, -0.00889952,  0.00087376,  0.00993894,\n",
       "       -0.00526193, -0.0052252 , -0.00251813,  0.00856714, -0.00082937,\n",
       "        0.00941661, -0.00834268,  0.00899568, -0.00968952,  0.00919415,\n",
       "        0.00187411,  0.00266565, -0.00583919,  0.00844736,  0.00604254,\n",
       "       -0.00546242, -0.00948008,  0.00697376, -0.00552541, -0.00929961,\n",
       "        0.00260027, -0.00496849, -0.00735783,  0.0097235 , -0.00158981],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the vector of against\n",
    "vector=model.wv['against']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the similar words for few\n",
    "similar=model.wv.most_similar('few')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shan', 0.2567380964756012),\n",
       " ('these', 0.2171691209077835),\n",
       " ('that', 0.21442332863807678),\n",
       " ('through', 0.18634600937366486),\n",
       " ('so', 0.1653948873281479),\n",
       " ('their', 0.1604892909526825),\n",
       " ('my', 0.15342916548252106),\n",
       " ('ain', 0.1506490558385849),\n",
       " ('re', 0.13988706469535828),\n",
       " ('now', 0.1391236037015915)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07405047"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the similarity between being and few\n",
    "model.wv.similarity(w1='being',w2='few')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'few'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In the list which one doesn't match to each other\n",
    "model.wv.doesnt_match([\"few\",\"being\",\"same\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "document=\"Human Machine Interface for Lab NLP Computer application\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus=[\"Human Machine Interface for Lab NLP Computer application\",\n",
    "             \"Human is intelligence\",\n",
    "             \"Computer is an electronic machine\",\n",
    "             \"Time is precious\",\n",
    "            \"Magnificient Thought\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist=set('for a of the and to in'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'and', 'for', 'in', 'of', 'the', 'to'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[[word for word in document.lower().\n",
    "       split() if word not in stoplist]\n",
    "      for document in text_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'machine', 'interface', 'lab', 'nlp', 'computer', 'application'],\n",
       " ['human', 'is', 'intelligence'],\n",
       " ['computer', 'is', 'an', 'electronic', 'machine'],\n",
       " ['time', 'is', 'precious'],\n",
       " ['magnificient', 'thought']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency=defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'machine', 'interface', 'lab', 'nlp', 'computer', 'application'],\n",
      " ['human', 'is', 'intelligence'],\n",
      " ['computer', 'is', 'an', 'electronic', 'machine'],\n",
      " ['time', 'is', 'precious'],\n",
      " ['magnificient', 'thought']]\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token]+=1\n",
    "processed_corpus=[[token for token in text if frequency[token]>1] for text in texts]\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<15 unique tokens: ['application', 'computer', 'human', 'interface', 'lab']...>\n"
     ]
    }
   ],
   "source": [
    "dictionary=corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'an': 9,\n",
      " 'application': 0,\n",
      " 'computer': 1,\n",
      " 'electronic': 10,\n",
      " 'human': 2,\n",
      " 'intelligence': 7,\n",
      " 'interface': 3,\n",
      " 'is': 8,\n",
      " 'lab': 4,\n",
      " 'machine': 5,\n",
      " 'magnificient': 13,\n",
      " 'nlp': 6,\n",
      " 'precious': 11,\n",
      " 'thought': 14,\n",
      " 'time': 12}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 1)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc=\"Human Computer Interaction\"\n",
    "new_vec=dictionary.doc2bow(new_doc.lower().split())\n",
    "new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
      " [(2, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 1), (8, 1), (9, 1), (10, 1)],\n",
      " [(8, 1), (11, 1), (12, 1)],\n",
      " [(13, 1), (14, 1)]]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus=[dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "from gensim import models\n",
    "tfidf=models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.tfidfmodel.TfidfModel at 0x17c07513548>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=\"human computer\".lower().split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['human', 'computer']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.7071067811865475), (2, 0.7071067811865475)]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_STF=corpora.Dictionary(simple_preprocess(line) for line in open(r\"sample.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0, 'can': 1, 'computer': 2, 'human': 3, 'in': 4, 'is': 5, 'it': 6, 'made': 7, 'most': 8, 'powerful': 9, 'the': 10, 'this': 11, 'use': 12, 'way': 13, 'we': 14, 'weapon': 15, 'world': 16}\n"
     ]
    }
   ],
   "source": [
    "print(dict_STF.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
